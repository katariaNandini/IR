{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdbW6zFVOexpU2GNuiMhmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katariaNandini/IR/blob/main/ir_ass2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-d383UKNxM2",
        "outputId": "80cbedc8-a09e-47e6-c4cb-e7fbeb531ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query (or 'exit' to quit): Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
            "Top ranked documents:\n",
            "  Document ID: 28\n",
            "  Document Name: zomato.txt\n",
            "  Content Preview:\n",
            "If you are a restaurant owner or marketing manager for a restaurant, you’ll love Zomato. But what is Zomato, exactly, and why would you love it? We’ll introduce you to the platform, how to set up your business account, and everything that Zomato has to offer to help you boost your business.\n",
            "\n",
            "  Score: 0.5903\n",
            "  Link to Document: https://drive.google.com/file/d/1qCxhqPCobg-TzTzcrlRtprlhvZRDjqy0/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 30\n",
            "  Document Name: swiggy.txt\n",
            "  Content Preview:\n",
            "What Is Swiggy And How It’s Working?\n",
            "\n",
            "  Score: 0.3498\n",
            "  Link to Document: https://drive.google.com/file/d/1lTf2QiDCHu3syg6TqeGeTjupN4gtdWjI/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 16\n",
            "  Document Name: instagram.txt\n",
            "  Content Preview:\n",
            "What is instagram?\n",
            "\n",
            "  Score: 0.1611\n",
            "  Link to Document: https://drive.google.com/file/d/1cfhbL9uugppmXFwqCtwpgvpbx7k8iQtC/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 9\n",
            "  Document Name: messenger.txt\n",
            "  Content Preview:\n",
            "what is messenger?\n",
            "\n",
            "  Score: 0.1592\n",
            "  Link to Document: https://drive.google.com/file/d/10Iz3xS6CMbbkFkwDuENYEKoHrGh47WHZ/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 11\n",
            "  Document Name: youtube.txt\n",
            "  Content Preview:\n",
            "What is youtube?\n",
            "\n",
            "  Score: 0.1315\n",
            "  Link to Document: https://drive.google.com/file/d/1Vyp9dRXbByFNjHUCWjx6IOT5fBSNSPSu/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 3\n",
            "  Document Name: reddit.txt\n",
            "  Content Preview:\n",
            "What is reddit?\n",
            "\n",
            "  Score: 0.1269\n",
            "  Link to Document: https://drive.google.com/file/d/1RDdQqOcbVrmzPYIDQ6R1t7QXWrvzI68P/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "Enter your query (or 'exit' to quit): Warwickshire, came from an ancient family and was the heiress to some land\n",
            "Top ranked documents:\n",
            "  Document ID: 5\n",
            "  Document Name: shakespeare.txt\n",
            "  Content Preview:\n",
            "what is shakespeare?\n",
            "\n",
            "  Score: 0.4015\n",
            "  Link to Document: https://drive.google.com/file/d/1EEjfP9vyh8uzq3-N-TF95i_T91ZOF1On/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 10\n",
            "  Document Name: levis.txt\n",
            "  Content Preview:\n",
            "what is levis?\n",
            "\n",
            "  Score: 0.0844\n",
            "  Link to Document: https://drive.google.com/file/d/1i_XUVOQDZOvH21vVDAzPMFe3Db-60jIX/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 24\n",
            "  Document Name: nike.txt\n",
            "  Content Preview:\n",
            "What is nike?\n",
            "\n",
            "  Score: 0.0615\n",
            "  Link to Document: https://drive.google.com/file/d/1IKOAeE6R_lYM79KcCg1R69jaZqFESi4T/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 4\n",
            "  Document Name: Adobe.txt\n",
            "  Content Preview:\n",
            "what is adobe?\n",
            "\n",
            "  Score: 0.0537\n",
            "  Link to Document: https://drive.google.com/file/d/1PRdJlT5wixJd_FTe3moFqTlHbanAo_5_/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 28\n",
            "  Document Name: zomato.txt\n",
            "  Content Preview:\n",
            "If you are a restaurant owner or marketing manager for a restaurant, you’ll love Zomato. But what is Zomato, exactly, and why would you love it? We’ll introduce you to the platform, how to set up your business account, and everything that Zomato has to offer to help you boost your business.\n",
            "\n",
            "  Score: 0.0499\n",
            "  Link to Document: https://drive.google.com/file/d/1qCxhqPCobg-TzTzcrlRtprlhvZRDjqy0/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "  Document ID: 17\n",
            "  Document Name: huawei.txt\n",
            "  Content Preview:\n",
            "what is huawei?\n",
            "\n",
            "  Score: 0.0474\n",
            "  Link to Document: https://drive.google.com/file/d/1uRFepmUklcGoydXm7CNOCbyApWmyJ3zu/view?usp=sharing\n",
            "--------------------------------------------------\n",
            "Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, name: str, content: str):\n",
        "        self.name = name\n",
        "        self.content = content\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return words\n",
        "\n",
        "# Function to load documents from a zip file\n",
        "def load_documents(zip_path: str, extract_to: str) -> Dict[int, Document]:\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    # Verify extraction\n",
        "    extracted_files = []\n",
        "    for root, dirs, files in os.walk(extract_to):\n",
        "        for file in files:\n",
        "            extracted_files.append(os.path.join(root, file))\n",
        "\n",
        "    # Load the documents\n",
        "    docs = {}\n",
        "    for i, file_path in enumerate(extracted_files):\n",
        "        if file_path.endswith('.txt'):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read().strip()\n",
        "                if content:  # Only add non-empty files\n",
        "                    docs[i + 1] = Document(name=os.path.basename(file_path), content=content)\n",
        "\n",
        "    return docs\n",
        "\n",
        "# Function to build the inverted index for ranked retrieval\n",
        "def build_inverted_index(docs: Dict[int, Document]) -> Tuple[Dict[str, Dict[int, int]], Dict[int, float]]:\n",
        "    inverted_index = defaultdict(lambda: defaultdict(int))\n",
        "    doc_lengths = defaultdict(float)\n",
        "\n",
        "    for doc_id, document in docs.items():\n",
        "        words = preprocess(document.content)\n",
        "        word_freq = defaultdict(int)\n",
        "        for word in words:\n",
        "            word_freq[word] += 1\n",
        "\n",
        "        # Compute TF and accumulate document lengths for normalization\n",
        "        doc_length = 0\n",
        "        for word, freq in word_freq.items():\n",
        "            inverted_index[word][doc_id] = freq\n",
        "            doc_length += (1 + math.log10(freq)) ** 2\n",
        "        doc_lengths[doc_id] = math.sqrt(doc_length)\n",
        "\n",
        "    return inverted_index, doc_lengths\n",
        "\n",
        "# Function to handle ranked retrieval\n",
        "def ranked_retrieval(query: str,\n",
        "                     inverted_index: Dict[str, Dict[int, int]],\n",
        "                     doc_lengths: Dict[int, float],\n",
        "                     total_docs: int,\n",
        "                     top_n: int = 6) -> Dict[int, float]:\n",
        "\n",
        "    # Preprocess the query\n",
        "    query_terms = preprocess(query)\n",
        "    query_term_freq = defaultdict(int)\n",
        "\n",
        "    # Calculate term frequency for the query\n",
        "    for term in query_terms:\n",
        "        query_term_freq[term] += 1\n",
        "\n",
        "    # Calculate query weights using the ltc scheme\n",
        "    query_weights = {}\n",
        "    for term, freq in query_term_freq.items():\n",
        "        query_weights[term] = 1 + math.log10(freq)\n",
        "\n",
        "    # Score the documents\n",
        "    doc_scores = defaultdict(float)\n",
        "    for term, query_weight in query_weights.items():\n",
        "        if term in inverted_index:\n",
        "            doc_freqs = inverted_index[term]\n",
        "            idf = math.log10(total_docs / len(doc_freqs)) if len(doc_freqs) > 0 else 0\n",
        "            for doc_id, term_freq in doc_freqs.items():\n",
        "                tf = 1 + math.log10(term_freq)\n",
        "                doc_scores[doc_id] += tf * idf * query_weight\n",
        "\n",
        "    # Normalize the document scores\n",
        "    for doc_id in doc_scores:\n",
        "        if doc_lengths[doc_id] > 0:\n",
        "            doc_scores[doc_id] /= doc_lengths[doc_id]\n",
        "\n",
        "    # Return only the top N documents\n",
        "    return dict(sorted(doc_scores.items(), key=lambda item: item[1], reverse=True)[:top_n])\n",
        "\n",
        "# Function to display document details\n",
        "def display_document(doc_id: int, docs: Dict[int, Document], file_id_mapping: Dict[int, str], score: float):\n",
        "    doc = docs[doc_id]\n",
        "    lines = doc.content.split('\\n')\n",
        "    first_two_lines = '\\n'.join(lines[:2])\n",
        "\n",
        "    # Get the file ID for the document\n",
        "    file_id = file_id_mapping.get(doc_id, None)\n",
        "    if file_id:\n",
        "        # Construct the Google Drive link for the individual file\n",
        "        link = f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n",
        "    else:\n",
        "        link = \"Link not available\"\n",
        "\n",
        "    print(f\"  Document ID: {doc_id}\")\n",
        "    print(f\"  Document Name: {doc.name}\")\n",
        "    print(f\"  Content Preview:\\n{first_two_lines}\")\n",
        "    print(f\"  Score: {score:.4f}\")  # Display the score with four decimal places\n",
        "    print(f\"  Link to Document: {link}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Main function to run the ranked retrieval system\n",
        "def main():\n",
        "    corpus_zip_path = 'Corpus.zip'\n",
        "    corpus_dir = 'Corpus'\n",
        "\n",
        "    # Load documents\n",
        "    docs = load_documents(corpus_zip_path, corpus_dir)\n",
        "\n",
        "    if not docs:\n",
        "        print(\"No documents loaded. Please check the files and their content.\")\n",
        "        return\n",
        "\n",
        "    # Build inverted index and calculate document lengths\n",
        "    inverted_index, doc_lengths = build_inverted_index(docs)\n",
        "    total_docs = len(docs)\n",
        "\n",
        "    # Example mapping for file IDs\n",
        "    file_id_mapping = {\n",
        "        1: '17gtGk9isJIjMOwJUhrvScWIkBeIivQvk',\n",
        "        2: '1Bi7UM3yJfro4Tu3-g3fm5Y2XZ8YeZHg5',\n",
        "        3: '1RDdQqOcbVrmzPYIDQ6R1t7QXWrvzI68P',\n",
        "        4: '1PRdJlT5wixJd_FTe3moFqTlHbanAo_5_',\n",
        "        5: '1EEjfP9vyh8uzq3-N-TF95i_T91ZOF1On',\n",
        "        6: '1-zWgMQAwSOCsmQGYzkyAINNZdGZaG_9N',\n",
        "        7: '1iiaWgK1Ic8jxQCFTDoAJwfEPOCpBcHyb',\n",
        "        8: '1RHmqiyJN3dsr0eLJwZaN_7cqJf44u4mQ',\n",
        "        9: '10Iz3xS6CMbbkFkwDuENYEKoHrGh47WHZ',\n",
        "        10: '1i_XUVOQDZOvH21vVDAzPMFe3Db-60jIX',\n",
        "        11: '1Vyp9dRXbByFNjHUCWjx6IOT5fBSNSPSu',\n",
        "        12: '1SU3qe7WPduaiOWQiEWKjVgWWNJJG2P8l', #paypal\n",
        "        13: '1myzc16SQiItwv7Fu34xKPdzhEAg8WMf_', #ig\n",
        "        14: '1YdVx3j2U1QIUqjApzzu_7Vc0yhwdN4W0', #volkswagen\n",
        "        15: '1gym8q9fuilqyNOZ_MbdTriQKiF4fs21T', #hp\n",
        "        16: '1cfhbL9uugppmXFwqCtwpgvpbx7k8iQtC', #spotify\n",
        "        17: '1uRFepmUklcGoydXm7CNOCbyApWmyJ3zu', #uber\n",
        "        18: '1u32cxm6JkQXMttqcgpKtOaW9JF_io2HZ', #utube\n",
        "        19: '1jTtyGhZ5Nt0r5H5P4q9SCMM4EKNocFbq', #motorola\n",
        "        20: '1L8cqu31UdqfUlSl2BUPZm2WgcA7KWXhW', #yahoo\n",
        "        21: '1Yf9ypzXtV0GVRzSROA0Rf5lDlzEZdurG', #lenevo\n",
        "        22: '1q628g8gPoXhQwQophtXpa1EOfLTia6Yr', #dell\n",
        "        23: '1NXxLhKMWLjFZw-OHi2omCUHneE65MEKy', #levis\n",
        "        24: '1IKOAeE6R_lYM79KcCg1R69jaZqFESi4T', #whatsapp\n",
        "        25: '1eJefZuegIKYKtO_Ui6Ft613HjNy4gjNa', #bing\n",
        "        26: '1UM0HfKDquiYfQUYPMb0OefYXuO_pU-py', #calender\n",
        "        27: '1P7xYaV89N-4Hrj_o2mh0VOZDbvDuKYKn', #ia\n",
        "        28: '1qCxhqPCobg-TzTzcrlRtprlhvZRDjqy0', #mck\n",
        "        29: '1RREKbmPEKwVh6A7A2FzI9oXw4eAIn4aG', #coca-cola\n",
        "        30: '1lTf2QiDCHu3syg6TqeGeTjupN4gtdWjI', #red bull\n",
        "        31: '1GyV9b5-dUwH0H5b7ED0Gy0RrGx0MKp2P', #pepsi\n",
        "        32: '1ihkdXfowgn_G0X_15fC6H9fsC8pLN3NZ', #mtv\n",
        "        33: '1tHcRPbDq2aQGAxqA-zOhBlqZlz4h1ZB5', #imdb\n",
        "        34: '1sQXLYZ6g9DJHMBF-Fk97pbwAFWBNg0Id', #ebay\n",
        "        35: '1RQZWV4HEmP7Y4FCu0QzWPKZ4HvGPdPvm', #twitch\n",
        "        36: '1IBMFYSrMTcIRzeR-2msWr9tUV4opI6wI', #craigslist\n",
        "        37: '1DUby7sNxMeDqJ_kKZFd3heJIV9U_a5I8', #google\n",
        "        38: '1uCwNBkEKFl5dKzEKpZK0Ab0TPn53Bl7C', #yellow pages\n",
        "        39: '1MYYGZRaTz9R2N-lDp0FhPU4D5YvOePf2', #twitter\n",
        "        40: '1OfrYB87DEUPu5CAdX6lYcKcKAGuQ-D4D', #credit karma\n",
        "        41: '1W9osaf5vDpk5I6gFtuTG16yD6J6Kbi2u', #monster\n",
        "        42: '1f0kmftCR9xn3giGRqUmOEy7Z9_aHiNIi', #facebook\n",
        "        43: '1LFwa-RZstduYgG3ZpAwAcH6pV2O0xDb5', #travelocity\n",
        "        44: '1ue0TWp4Fb68W_XcELvBboGvGyOon8z4F', #yelp\n",
        "        45: '1_nxJTIHQdeHjBE6btF5Fz4QQdJK0LqT8', #cars\n",
        "        46: '1hyRV1nd_hOUop9pL23J5tt7G-xhiF1is', #expedia\n",
        "        47: '1-uRRBpm8S61FWT3t9UJ-zU1klun9o5E', #cheap\n",
        "        48: '1DhT0w8NYFx53b2OWac_X8izDOUu93e5x', #last fm\n",
        "        49: '1L9Dqu4lMGLm89Sx49Fu-a7ZJg5gYtOnB', #foxnews\n",
        "        50: '1ScWlCTk7cfZfjxi2C7cR9C99KpfuLV3Z', #fandom\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"Enter your query (or 'exit' to quit): \")\n",
        "        if user_query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Perform ranked retrieval\n",
        "        doc_scores = ranked_retrieval(user_query, inverted_index, doc_lengths, total_docs)\n",
        "\n",
        "        # Display results\n",
        "        if doc_scores:\n",
        "            print(\"Top ranked documents:\")\n",
        "            for doc_id, score in doc_scores.items():\n",
        "                display_document(doc_id, docs, file_id_mapping, score)\n",
        "        else:\n",
        "            print(\"No documents found matching the query.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}
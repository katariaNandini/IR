{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8vNbGy3q5syPmhiqO/bzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katariaNandini/IR/blob/main/ir_ass2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-d383UKNxM2",
        "outputId": "e73c2b1b-7837-4d07-9c71-b3871f6539be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your search query (or 'exit' to quit): Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
            "\n",
            "Query: Developing your Zomato business account and profile is a great way to boost your restaurant’s online reputation\n",
            "Top 5 documents:\n",
            "  zomato.txt (Score: 0.5903)\n",
            "  swiggy.txt (Score: 0.3498)\n",
            "  instagram.txt (Score: 0.1611)\n",
            "  messenger.txt (Score: 0.1592)\n",
            "  youtube.txt (Score: 0.1315)\n",
            "----------------------------------------\n",
            "Enter your search query (or 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  # Importing stopwords from NLTK\n",
        "from nltk.stem import WordNetLemmatizer  # Importing lemmatizer for word normalization\n",
        "from collections import defaultdict  # Default dictionary for easier dictionary operations\n",
        "from typing import List, Dict, Tuple  # Importing typing hints for better code understanding\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Preprocessing function to clean and normalize the text\n",
        "def preprocess(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    This function performs text preprocessing including:\n",
        "    - Converting text to lowercase\n",
        "    - Tokenizing the text\n",
        "    - Removing non-alphabetic characters and stop words\n",
        "    - Lemmatizing the words\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    words = nltk.word_tokenize(text)  # Tokenize the text into words\n",
        "    words = [word for word in words if word.isalpha()]  # Remove non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words\n",
        "    return words\n",
        "\n",
        "# Function to extract documents from a zip file\n",
        "def extract_documents(corpus_zip_path: str, corpus_dir: str) -> Tuple[Dict[int, str], Dict[int, str]]:\n",
        "    \"\"\"\n",
        "    Extracts documents from the provided zip file path and stores them in the specified directory.\n",
        "    - Returns a dictionary of document IDs mapped to their content, and a mapping of document IDs to filenames.\n",
        "    \"\"\"\n",
        "    docs = {}\n",
        "    file_to_doc_id = {}\n",
        "\n",
        "    # Create directory if it does not exist\n",
        "    if not os.path.exists(corpus_dir):\n",
        "        os.makedirs(corpus_dir)\n",
        "\n",
        "    # Extract zip file to the directory\n",
        "    with zipfile.ZipFile(corpus_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(corpus_dir)\n",
        "\n",
        "    # Check for the 'Corpus' subdirectory and load documents\n",
        "    docs_dir = os.path.join(corpus_dir, 'Corpus')\n",
        "    if os.path.exists(docs_dir):\n",
        "        for i, filename in enumerate(os.listdir(docs_dir)):\n",
        "            if filename.endswith('.txt'):\n",
        "                file_path = os.path.join(docs_dir, filename)\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "                    docs[i + 1] = content  # Assign doc ID starting from 1\n",
        "                    file_to_doc_id[i + 1] = filename  # Map doc ID to filename\n",
        "    else:\n",
        "        print(f\"Directory {docs_dir} does not exist.\")\n",
        "        return {}, {}\n",
        "\n",
        "    return docs, file_to_doc_id\n",
        "\n",
        "# Function to build the inverted index for ranked retrieval\n",
        "def build_inverted_index(docs: Dict[int, str]) -> Tuple[Dict[str, Dict[int, int]], Dict[int, float]]:\n",
        "    \"\"\"\n",
        "    Builds an inverted index from the provided documents.\n",
        "    - Returns the inverted index and a dictionary of document lengths for normalization.\n",
        "    \"\"\"\n",
        "    inverted_index = defaultdict(lambda: defaultdict(int))\n",
        "    doc_lengths = defaultdict(float)\n",
        "\n",
        "    for doc_id, content in docs.items():\n",
        "        words = preprocess(content)  # Preprocess the document content\n",
        "        word_freq = defaultdict(int)\n",
        "        for word in words:\n",
        "            word_freq[word] += 1  # Calculate word frequency in the document\n",
        "\n",
        "        # Compute Term Frequency (TF) and accumulate document lengths for normalization\n",
        "        doc_length = 0\n",
        "        for word, freq in word_freq.items():\n",
        "            inverted_index[word][doc_id] = freq\n",
        "            doc_length += (1 + math.log10(freq)) ** 2  # Calculate weighted term frequency\n",
        "        doc_lengths[doc_id] = math.sqrt(doc_length)  # Store document length\n",
        "\n",
        "    return inverted_index, doc_lengths\n",
        "\n",
        "# Function to handle ranked retrieval\n",
        "def ranked_retrieval(query: str, inverted_index: Dict[str, Dict[int, int]], doc_lengths: Dict[int, float], total_docs: int) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Performs ranked retrieval on the given query using the inverted index and document lengths.\n",
        "    - Returns a dictionary of document IDs mapped to their similarity scores.\n",
        "    \"\"\"\n",
        "    query_terms = preprocess(query)  # Preprocess the query\n",
        "    query_term_freq = defaultdict(int)\n",
        "\n",
        "    # Calculate term frequency for the query\n",
        "    for term in query_terms:\n",
        "        query_term_freq[term] += 1\n",
        "\n",
        "    # Calculate query weights using the ltc scheme (logarithmic term frequency)\n",
        "    query_weights = {}\n",
        "    for term, freq in query_term_freq.items():\n",
        "        query_weights[term] = 1 + math.log10(freq)\n",
        "\n",
        "    # Score the documents based on query weights and document frequencies\n",
        "    doc_scores = defaultdict(float)\n",
        "    for term, query_weight in query_weights.items():\n",
        "        if term in inverted_index:\n",
        "            doc_freqs = inverted_index[term]\n",
        "            idf = math.log10(total_docs / len(doc_freqs)) if len(doc_freqs) > 0 else 0  # Calculate IDF\n",
        "            for doc_id, term_freq in doc_freqs.items():\n",
        "                tf = 1 + math.log10(term_freq)  # Calculate term frequency for the document\n",
        "                doc_scores[doc_id] += tf * idf * query_weight\n",
        "\n",
        "    # Normalize the document scores by their lengths\n",
        "    for doc_id in doc_scores:\n",
        "        if doc_lengths[doc_id] > 0:\n",
        "            doc_scores[doc_id] /= doc_lengths[doc_id]\n",
        "\n",
        "    # Return sorted document scores in descending order\n",
        "    return dict(sorted(doc_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Main function to run the ranked retrieval system\n",
        "def main():\n",
        "    corpus_zip_path = 'Corpus.zip'  # Path to the corpus zip file\n",
        "    corpus_dir = 'Corpus'  # Directory where the corpus will be extracted\n",
        "\n",
        "    # Extract documents from the corpus zip file\n",
        "    docs, file_to_doc_id = extract_documents(corpus_zip_path, corpus_dir)\n",
        "\n",
        "    if not docs:\n",
        "        print(\"No documents loaded. Please check the files and their content.\")\n",
        "        return\n",
        "\n",
        "    # Build inverted index and calculate document lengths\n",
        "    inverted_index, doc_lengths = build_inverted_index(docs)\n",
        "    total_docs = len(docs)\n",
        "\n",
        "    # Loop to continuously accept user queries until 'exit' is entered\n",
        "    while True:\n",
        "        query = input(\"Enter your search query (or 'exit' to quit): \")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Perform ranked retrieval for the given query\n",
        "        doc_scores = ranked_retrieval(query, inverted_index, doc_lengths, total_docs)\n",
        "\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        if doc_scores:\n",
        "            print(\"Top 5 documents:\")\n",
        "            for doc_id in list(doc_scores.keys())[:5]:  # Display top 5 results\n",
        "                print(f\"  {file_to_doc_id[doc_id]} (Score: {doc_scores[doc_id]:.4f})\")\n",
        "        else:\n",
        "            print(\"No matching documents.\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Entry point for the program\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}
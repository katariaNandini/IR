{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcxJ5g+iP4sedIwn/r1eCc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katariaNandini/IR/blob/main/vsm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6K0HtfqOgYB",
        "outputId": "d96f2cdb-5777-4413-e96a-09edd58c75db"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query: Developing your Zomato business account and profile is a great way to boost your restaurantâ€™s online reputation\n",
            "Top 5 documents:\n",
            "  zomato.txt (Score: 0.1755)\n",
            "  swiggy.txt (Score: 0.1039)\n",
            "  instagram.txt (Score: 0.0476)\n",
            "  messenger.txt (Score: 0.0471)\n",
            "  youtube.txt (Score: 0.0389)\n",
            "----------------------------------------\n",
            "\n",
            "Query: Warwickshire, came from an ancient family and was the heiress to some land\n",
            "Top 5 documents:\n",
            "  shakespeare.txt (Score: 0.1640)\n",
            "  levis.txt (Score: 0.0360)\n",
            "  nike.txt (Score: 0.0272)\n",
            "  huawei.txt (Score: 0.0209)\n",
            "  zomato.txt (Score: 0.0202)\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import math\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word.isalpha()]  # Keep only alphabetic words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize the words\n",
        "    return words\n",
        "\n",
        "# Function to extract documents\n",
        "def extract_documents(corpus_zip_path: str, corpus_dir: str) -> Tuple[Dict[int, str], Dict[int, str]]:\n",
        "    docs = {}\n",
        "    file_to_doc_id = {}\n",
        "\n",
        "    if not os.path.exists(corpus_dir):\n",
        "        os.makedirs(corpus_dir)\n",
        "\n",
        "    with zipfile.ZipFile(corpus_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(corpus_dir)\n",
        "\n",
        "    docs_dir = os.path.join(corpus_dir, 'Corpus')\n",
        "    if os.path.exists(docs_dir):\n",
        "        for i, filename in enumerate(os.listdir(docs_dir)):\n",
        "            if filename.endswith('.txt'):\n",
        "                file_path = os.path.join(docs_dir, filename)\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    content = file.read()\n",
        "                    docs[i + 1] = content  # Assign doc ID starting from 1\n",
        "                    file_to_doc_id[i + 1] = filename  # Map doc ID to filename\n",
        "    else:\n",
        "        print(f\"Directory {docs_dir} does not exist.\")\n",
        "        return {}, {}\n",
        "\n",
        "    return docs, file_to_doc_id\n",
        "\n",
        "# Function to build the inverted index for ranked retrieval\n",
        "def build_inverted_index(docs: Dict[int, str]) -> Tuple[Dict[str, Dict[int, int]], Dict[int, float]]:\n",
        "    inverted_index = defaultdict(lambda: defaultdict(int))\n",
        "    doc_lengths = defaultdict(float)\n",
        "\n",
        "    for doc_id, content in docs.items():\n",
        "        words = preprocess(content)\n",
        "        word_freq = defaultdict(int)\n",
        "        for word in words:\n",
        "            word_freq[word] += 1\n",
        "\n",
        "        # Compute TF and accumulate document lengths for normalization\n",
        "        doc_length = 0\n",
        "        for word, freq in word_freq.items():\n",
        "            tf = 1 + math.log10(freq)  # log-based term frequency\n",
        "            inverted_index[word][doc_id] = tf  # Store TF directly\n",
        "            doc_length += tf ** 2\n",
        "        doc_lengths[doc_id] = math.sqrt(doc_length)  # Store document length for normalization\n",
        "\n",
        "    return inverted_index, doc_lengths\n",
        "\n",
        "# Function to handle ranked retrieval\n",
        "def ranked_retrieval(query: str, inverted_index: Dict[str, Dict[int, float]], doc_lengths: Dict[int, float], total_docs: int) -> Dict[int, float]:\n",
        "    query_terms = preprocess(query)\n",
        "    query_term_freq = defaultdict(int)\n",
        "\n",
        "    # Calculate term frequency for the query\n",
        "    for term in query_terms:\n",
        "        query_term_freq[term] += 1\n",
        "\n",
        "    # Calculate query weights using ltc scheme (logarithmic term frequency)\n",
        "    query_weights = {}\n",
        "    query_length = 0\n",
        "    for term, freq in query_term_freq.items():\n",
        "        query_weights[term] = 1 + math.log10(freq)  # Logarithmic frequency\n",
        "        query_length += query_weights[term] ** 2  # Sum of squares for query length normalization\n",
        "\n",
        "    query_length = math.sqrt(query_length) if query_length > 0 else 1  # Avoid division by zero\n",
        "\n",
        "    # Score the documents\n",
        "    doc_scores = defaultdict(float)\n",
        "    for term, query_weight in query_weights.items():\n",
        "        if term in inverted_index:\n",
        "            doc_freqs = inverted_index[term]\n",
        "            idf = math.log10(total_docs / len(doc_freqs)) if len(doc_freqs) > 0 else 0\n",
        "            for doc_id, term_weight in doc_freqs.items():\n",
        "                doc_scores[doc_id] += term_weight * idf * query_weight  # TF-IDF * query weight\n",
        "\n",
        "    # Normalize the document scores\n",
        "    for doc_id in doc_scores:\n",
        "        if doc_lengths[doc_id] > 0:\n",
        "            doc_scores[doc_id] /= doc_lengths[doc_id]  # Normalize by document length\n",
        "        doc_scores[doc_id] /= query_length  # Normalize by query length\n",
        "\n",
        "    return dict(sorted(doc_scores.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# Main function to run the ranked retrieval system\n",
        "def main():\n",
        "    corpus_zip_path = 'Corpus.zip'\n",
        "    corpus_dir = 'Corpus'\n",
        "\n",
        "    # Extract documents\n",
        "    docs, file_to_doc_id = extract_documents(corpus_zip_path, corpus_dir)\n",
        "\n",
        "    if not docs:\n",
        "        print(\"No documents loaded. Please check the files and their content.\")\n",
        "        return\n",
        "\n",
        "    # Build inverted index and calculate document lengths\n",
        "    inverted_index, doc_lengths = build_inverted_index(docs)\n",
        "    total_docs = len(docs)\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your search query (or 'exit' to quit): \")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Perform ranked retrieval\n",
        "        doc_scores = ranked_retrieval(query, inverted_index, doc_lengths, total_docs)\n",
        "\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        if doc_scores:\n",
        "            print(\"Top 5 documents:\")\n",
        "            for doc_id in list(doc_scores.keys())[:5]:  # Show top 5 results\n",
        "                print(f\"  {file_to_doc_id[doc_id]} (Score: {doc_scores[doc_id]:.4f})\")\n",
        "        else:\n",
        "            print(\"No matching documents.\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Entry point for the program\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}